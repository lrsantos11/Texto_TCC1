\documentclass[12pt,a4paper]{scrartcl}
\usepackage[brazil]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[onehalfspacing]{setspace}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage[brazilian]{cleveref}

\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{indentfirst}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator*{\argmax}{arg\, max}
\DeclareMathOperator*{\argmin}{arg\, min}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\ones}{ones}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\interior}{int}

\def\Xset{\mathcal{X}}
\def\Yset{\mathcal{Y}}
\def\Hset{\mathcal{H}}
\def\RR{\mathds{R}}
\def\xbar{\bar{x}}
\def\wbar{\bar{w}}
\def\bbar{\bar{b}}


\newtheorem{teorema}{Teorema}%ambientes em itálico
\newtheorem{prop}{Proposição}
\newtheorem{teo}{Teorema}
\newtheorem{lema}{Lema}

\theoremstyle{definition}%ambientes normais
\newtheorem{exem}{Exemplo}[subsection]
\newtheorem{defi}{Definição}
\newtheorem{obs}{Observação}	

\usepackage{csquotes}
\usepackage[
backend=biber,
style=numeric-comp, noerroretextools=true, sorting=nty,
]{biblatex}
\addbibresource{referencias.bib}

 \let\etoolboxforlistloop\forlistloop % save the good meaning of \forlistloop
 \usepackage{autonum}
 \let\forlistloop\etoolboxforlistloop % restore the good meaning of \forlistloop
 
 
\begin{document}

\title{Métodos de Otimização e Máquinas de Vetores Suporte} 
\author{ \normalfont Paula Cristina Rohr Ertel\thanks{Acadêmica do curso de Licenciatura em Matemática/UFSC-Blumenau} \\ \small Orientador: Luiz Rafael dos Santos \\ \small Universidade Federal de Santa Catarina - Campus Blumenau}
\date{\small 18 de Novembro de 2019}
\subtitle{Qualificação de Trabalho de Conclusão de Curso}
\maketitle

\section{Introdução às Máquinas de Vetores Suporte}

A Aprendizagem de Máquina (do inglês \textit{Machine Learning}) é o estudo do uso de técnicas computacionais para automaticamente detectar padrões em dados e usá-los para fazer predições e tomar decisões. De acordo com \textcite{Evelin2017}, existem dois tipos de Aprendizagem de Máquina, a aprendizagem supervisionada, em que a partir de um conjunto de dados de entrada e saída a máquina constrói um modelo que deduz a saída para novas entradas, e a não supervisionada, na qual a máquina cria sua própria solução. 

A aprendizagem supervisionada é composta por uma etapa denominada fase de treinamento, na qual é dado um conjunto de treinamento formado por vários dados de entrada e saída que funcionam como exemplos, a partir dos quais a máquina detecta padrões e cria um modelo para deduzir a saída de novos dados. Após essa fase novas entradas são testadas, denominadas conjunto de teste, no intuito de analisar se a máquina está gerando as saídas corretas. Algumas técnicas para aprendizagem de máquina supervisionada são as Máquinas de Vetores Suporte, Regressão Linear, Regressão Logística e Redes Neurais. Enquanto que a \textit{Singular Value Decomposition} (SVD), Clusterização e Análise de Componentes Principais \cite{Evelin2017} são exemplos de técnicas para a aprendizagem não supervisionada. 

As Máquinas de Vetores Suporte (SVM, do inglês \textit{Support Vector Machine}), conforme mencionado por \textcite{Evelin2017}, são indicadas nos casos em que ocorrem dados de dimensões elevadas e com altos níveis de ruídos, além de apresentar uma boa capacidade de generalização. Esta técnica pode ser aplicada tanto para problemas de regressão como de classificação. Segundo \textcite{Evelin2017}, essa técnica foi desenvolvida por Vladimir Vapnik, Bernhard Boser, Isabelle Guyon e Corrina Cortes, com base na Teoria de Aprendizagem Estatística. Algumas aplicações de SVM em problemas práticos são o reconhecimento facial, leitura de placas automotivas e detecção de spam.

Agora, vamos formular matematicamente o problema de classificação utilizando as Máquinas de Vetores Suporte. Para tanto, considere um conjunto de dados, pertencentes a duas classes distintas, conforme \Cref{fig1}.


\begin{figure}[!h] 
\centering
\begin{subfigure}[h]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{SVM_linear}
\caption{Linear. \label{fig1:a}}
\end{subfigure}
\begin{subfigure}[!h]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{SVM_flexivel}
	\caption{Flexível. \label{fig1:b}}
\end{subfigure}
\begin{subfigure}[!h]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{SVM_naolinear}
	\caption{Não Linear. \label{fig1:c}}
\end{subfigure}
\caption{Dados lineares, com margem flexível e não lineares. \label{fig1}\\ Fonte: \textcite{Evelin2017}}
\end{figure}

Observe que na \Cref{fig1:a} os dados podem ser classificados corretamente através de uma reta. Já na \Cref{fig1:b} é possível encontrar uma reta que separa alguns poucos dados, porém incorretamente. E na \Cref{fig1:c} não é possível classificar os dados como nos casos anteriores. Nestes exemplos temos representados os três casos de SVM: o linear com margem rígida, o linear com margem flexível e o não linear, respectivamente.

A modelagem do problema de classificação, utilizando a técnica de SVM, consiste em encontrar um hiperplano ótimo que melhor separe os dados de entrada $x^i$ em duas saídas $y_i$ através de uma função de decisão. Matematicamente, mostraremos que trata-se um problema de programação quadrática convexa com restrições lineares, que pode ser formulado como
\[
\begin{aligned}
\min_{w,b} & \quad f(w) \\
\text{s.a.} &  \quad g(w,b) \leq 0, \end{aligned}
\]
com $w\in \RR^n$ e $b\in \RR $, em que $f: \RR^n \rightarrow \RR$ é uma função quadrática e $g: \RR^{n+1} \rightarrow \RR^m$ é linear. Note também que $f$ e $g$ são continuamente diferenciáveis.



Para formular matematicamente o problema de classificação, considere os conjuntos de entrada $\Xset =\{x^1, \ldots , x^m \} \subset \RR^n$ e de treinamento $\Yset=\{(x^1, y_1), \ldots , (x^m, y_m)\mid x^i \in \Xset \, e \, y_i \in \{-1,1\}\}$, com a partição 
\[ \label{conj1}
\Xset ^{+} =\{x^i \in \Xset\mid y_i=1\} \quad e \quad \Xset^{-}=\{x^i \in \Xset\mid y_i=-1\},
\]
dos conjuntos formados pelos atributos pertencentes às classes positiva e negativa, respectivamente.

\begin{defi} Considere um vetor não nulo $w\in \RR^n$ e um escalar $b\in \RR$. Um hiperplano com vetor normal $w$ e constante $b$ é um conjunto da forma $\Hset(w,b)=\{x\in \RR^n \mid w^{T}x+b=0\}$.
\end{defi}

O hiperplano $\Hset(w,b)$ divide o espaço $\RR^n$ em dois semiespaços, dados por
\[ \label{conj2}
\mathcal{S}^{+}=\{x\in \RR^n \mid w^{T}x+b\geq 0\} \quad e \quad \mathcal{S}^{-}=\{x\in \RR^n \mid w^{T}x+b\leq 0\}.
\]


Considere dois conjuntos de dados de treinamento representados no $\RR^2$ como na \Cref{fig2:a}, em que os pontos em azul representam a classe positiva, e os pontos em vermelho a classe negativa. Perceba na \Cref{fig2:b} que todos os hiperplanos representados separam corretamente os dados, porém nosso objetivo será encontrar o hiperplano que melhor separa esses dados, o qual está representado na \Cref{fig3:a} pela cor violeta. Logo, desejamos encontrar o hiperplano que possibilita a maior faixa que não contém nenhum dado, pois caso a faixa seja muito estreita pequenas perturbações no hiperplano ou no conjunto de dados podem resultar uma classificação incorreta. 
\begin{figure}[htbp] 
	\centering
	\begin{subfigure}[h]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{dados_treinamento}
		\caption{Dados de treinamento. \label{fig2:a}}
	\end{subfigure}
	\begin{subfigure}[h]{0.38\textwidth}
		\centering
		\includegraphics[width=\textwidth]{hiperplanos_separadores}
		\caption{Hiperplanos separadores. \label{fig2:b}}
	\end{subfigure}
\caption{Conjunto de Dados e Hiperplanos. \label{fig2}
	\\ Fonte: \textcite{Evelin2017}}
\end{figure}
\begin{figure}[hbtp] 
	\centering
	\begin{subfigure}[h]{0.38\textwidth}
		\centering
		\includegraphics[width=\textwidth]{hiperplano_otimo}
		\caption{Hiperplano ótimo. \label{fig3:a}}
	\end{subfigure}
	\begin{subfigure}[h]{0.38\textwidth}
		\centering
		\includegraphics[width=\textwidth]{maxima_margem}
		\caption{Máxima margem. \label{fig3:b}}	
	\end{subfigure}
\caption{Hiperplano Ótimo. \label{fig3}
\\ Fonte: \textcite{Evelin2017}}
\end{figure}


\begin{defi} \label{def1} Os conjuntos $\Xset^{+}, \Xset^{-} \subset \RR^n$ são ditos linearmente separáveis quando existem $w\in \RR^n$ e $b\in \RR$  tais que $w^{T}x+b>0$ para todo $x\in \Xset^{+}$ e $w^{T}x+b<0$ para todo $x\in \Xset^{-}$. O hiperplano $\Hset(w,b)$ é chamado hiperplano separador dos conjuntos $\Xset^{+}$ e $\Xset^{-}$.
\end{defi}


\begin{lema} \label{lema1} Suponha que os conjuntos $\Xset^{+}, \Xset^{-} \subset \RR^n$ são finitos e linearmente separáveis, com hiperplano separador $\Hset(w,b)$. Então, existem $\overline{w}\in \RR^n$ e $\overline{b}\in \RR$ tais que $\Hset(w,b)$ pode ser descrito por
\[
\wbar^{T}x+\bbar =0,
\]
satisfazendo
\begin{align}
\wbar^{T}x+\bbar &\geq 1, \text{ para todo } x\in \Xset^{+}, \label{eq1} \\
\wbar^{T}x+\bbar &\leq -1, \text{ para todo } x\in \Xset^{-}. \label{eq2}
\end{align}
\end{lema} 

\begin{proof}
Pela  \Cref{def1}, temos que existem $w\in \RR^n$ e $b\in \RR$ tais que
\begin{align}
w^{T}x+b &>0, \text{ para todo } x\in \Xset^{+}, \\
w^{T}x+b &<0, \text{ para todo } x\in \Xset^{-}.
\end{align}  

Como $\Xset^{+}\cup \Xset^{-}$ é um conjunto finito, podemos definir
\[ \gamma \coloneqq \min_{x\in \Xset^{+}\cup \Xset^{-}} \vert w^{T}x+b\vert  >0. \]

Portanto, para todo $x\in \Xset^{+}\cup \Xset^{-}$, $\gamma \leq \vert w^{T}x+b\vert$ e consequentemente, $\dfrac{\vert w^{T}x+b\vert }{\gamma} \geq 1$. Assim, para $x\in \Xset^{+}$ temos
\[ \dfrac{w^{T}x+b}{\gamma} = \dfrac{\vert w^{T}x+b\vert }{\gamma} \geq 1, \]
e para $x\in \Xset^{-}$, temos
\[- \dfrac{w^{T}x+b}{\gamma} = \dfrac{\vert w^{T}x+b\vert }{\gamma} \geq 1. \]

Logo, definindo $\wbar:=\dfrac{w}{\gamma}$ e $\bbar :=\dfrac{b}{\gamma}$, obtemos as desigualdades \eqref{eq1} e \eqref{eq2}. 

\end{proof}


A partir do \Cref{lema1} temos que $\Hset^{+}:=\{x\in \RR^n \mid w^{T}x+b= 1\}$ e $\Hset^{-}:=\{x\in \RR^n \mid w^{T}x+b= -1\}$ são os hiperplanos que definem a faixa que separa os conjuntos $\Xset^{+}$ e $\Xset^{-}$.

\begin{prop} \label{prop1} A projeção ortogonal de um vetor $\xbar\in \RR^n$ sobre um hiperplano afim $\Hset(w,b)$, é dada por
\[ \proj_{\Hset}(\xbar)= \xbar - \dfrac{w^{T}\xbar+b}{w^{T}w}w. \]
Além disso, a $\proj_{\Hset}(\xbar)$ satisfaz a menor distância.
\end{prop}

\begin{proof}
Sejam $w\in \RR^n$ o vetor normal ao hiperplano $\Hset(w,b)$, $\bar{z}\in \Hset(w,b)$ e $x^{*}$ a projeção ortogonal de $\xbar$ sobre $\Hset(w,b)$. Assim, temos que 
\[ \label{eq3} w^{T}(x^{*}-\bar{z})=0 \]
e
\[ \label{eq4} \xbar-x^{*}=\lambda w \Longrightarrow x^{*}=\xbar-\lambda w. \]

Substituindo \eqref{eq4} em \eqref{eq3}, obtemos
\begin{align}
0 &= w^{T}(\xbar-\lambda w-\overline{z}) \\
&= w^{T}\xbar-\lambda w^{T}w - w^{T}\bar{z}.
\end{align}

Resolvendo para $\lambda$ e como $w^{T}\bar{z} = -b$, temos
\[ \lambda =\dfrac{w^{T}\xbar-w^{T}\bar{z}}{w^{T}w} =\dfrac{w^{T}\xbar+b}{w^{T}w}. \]

Portanto, 
\[ x^{*}=\xbar-\dfrac{w^{T}\xbar+b}{w^{T}w}w . \]

Ademais, vamos provar que a $\proj_{\Hset}(\xbar)$ satisfaz a menor distância, isto é,
\[ \Vert\xbar-x^{*}\Vert_{2} \leq \Vert \xbar-x\Vert_{2}, \]
para todo $x\in \Hset(w,b)$.

De fato, tomando $u=\xbar-x^{*}$ e $v=x^{*}-x$ observe que 
\begin{align}
u^{T}v&= (\xbar-x^{*})^{T}(x^{*}-x) \\
&= (\xbar-\xbar+\lambda w)^{T}(x^{*}-x) \\
&= \lambda w^{T}(x^{*}-x) \\
&= \lambda (w^{T}x^{*}-w^{T}x) \\
&= \lambda (-b-(-b)) \\
&= 0.
\end{align}

Assim, temos
\[ \Vert u+v\Vert^{2}=\Vert u\Vert^{2} + 2u^{T}v + \Vert v\Vert^{2}=\Vert u\Vert^{2} + \Vert v\Vert^{2} , \]
ou seja,
\[ \Vert \xbar-x\Vert^{2}=\Vert \xbar-x^{*}\Vert^{2} + \Vert x^{*}-x\Vert^{2}. \]

\end{proof}


Utilizando a \Cref{prop1} podemos demonstrar o \Cref{lema2}, o qual estabelece a largura da faixa entre os hiperplanos separadores $\Hset^{+}$ e $\Hset^{-}$.

\begin{lema} \label{lema2} A distância entre os hiperplanos $\Hset^{+}$ e $\Hset^{-}$ é dada por $\dist(\Hset^{+}, \Hset^{-})=\dfrac{2}{\Vert w\Vert}$. 
\end{lema}
\begin{proof}
Considere um ponto arbitrário $\xbar\in \Hset^{+}$ e seja $x^{*}\in \Hset^{-}$ a projeção ortogonal de $\xbar$ sobre $\Hset^{-}$. Usando a \Cref{prop1}, temos
\[ \label{eq6} x^{*}= \proj_{\Hset^{-}}(\xbar)= \xbar - \dfrac{w^{T}\xbar+b+1}{\Vert w\Vert^{2}}w. \] 

Além disso, a distância entre dois conjuntos é definida por
\[ \dist(\Hset^{+}, \Hset^{-}):= \inf\{\Vert x^{+}-x^{-} \Vert : x^{+}\in \Hset^{+} \ \text{e} \ x^{-}\in \Hset^{-}\},
\]
e como a $\proj_{\Hset^{-}}(\xbar)$ satisfaz a menor distância entre $\xbar$ e $\Hset^{-}$, e $\Hset^{+}$ é paralelo a $\Hset^{-}$, temos que 
\[ \label{eq7} \dist(\Hset^{+},\Hset^{-})=\Vert \xbar-x^{*}\Vert. \]

Substituindo \eqref{eq6} em \eqref{eq7} obtemos
\begin{align} 
\dist(\Hset^{+},\Hset^{-}) &= \Vert \xbar-x^{*}\Vert \\
&= \left\Vert \xbar -\xbar +\dfrac{w^{T}\xbar+b+1}{\Vert w\Vert^{2}}w \right\Vert \\
&=  \dfrac{\vert w^{T}\xbar+b+1 \vert}{\Vert w\Vert^{2}} \Vert w\Vert \\
&= \dfrac{\vert w^{T}\xbar+b+1 \vert}{\Vert w\Vert},
\end{align}
e como $\xbar\in \Hset^{+}$,  $ w^{T}\xbar+b=1$ implica
\[  w^{T}\xbar =1-b, \]
concluindo que 
\begin{align} 
\dist(\Hset^{+},\Hset^{-})&= \dfrac{\vert 1-b+b+1 \vert}{\Vert w\Vert} \\
&= \dfrac{2}{\Vert w\Vert }. 
\end{align}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------

\subsection{Formulação Matemática do Problema de Classificação - Margem Rígida}

Encontrar o hiperplano que melhor separa os dados implica maximizar a largura da margem, isto é, maximizar $\dist(\Hset^{+} , \Hset^{-}) =\dfrac{2}{\Vert w\Vert }$. Isso equivale a minimizar seu inverso $\dfrac{1}{2}\Vert w\Vert $ ou ainda minimizar $\dfrac{1}{2}\Vert w\Vert^{2}$. De fato, seja $w^{*}=\argmax\dfrac{2}{\Vert w\Vert}$. Então, para todo $w\in \RR^n$,
\[ \dfrac{2}{\Vert w^{*}\Vert} \geq \dfrac{2}{\Vert w\Vert} \]
implica
\[ \label{eq8} \Vert w\Vert \geq \Vert w^{*}\Vert. \]
Logo, $w^{*}=\argmin\Vert w\Vert$. Além disso, como $\Vert \cdot \Vert$ é não negativa, elevando ao quadrado ambos os lados da desigualdade \eqref{eq8} temos que  $\Vert w\Vert^{2} \geq \Vert w^{*}\Vert^{2}$ implica
\[ \dfrac{1}{2}\Vert w\Vert^{2} \geq \dfrac{1}{2}\Vert w^{*}\Vert^{2}. \]

Portanto, 
\[ \argmax\dfrac{2}{\Vert w\Vert} = \argmin\dfrac{1}{2}\Vert w\Vert^2. \]

Ademais, como a faixa deve separar os dados das duas classes, as seguintes restrições devem ser satisfeitas
\begin{align}
w^{T}x+b &\geq 1 , \text{ para  todo } x\in \Xset^{+}, \\
w^{T}x+b &\leq -1 , \text{ para  todo } x\in \Xset^{-}.
\end{align}

Considerando que $\Xset^{+}=\{x^i \in \Xset\mid y_i=1\}$ e $\Xset^{-}=\{x^i \in \Xset \mid y_i=-1\}$, podemos reescrever as restrições acima de uma forma mais compacta 
\[ y_{i}(w^{T}x^{i}+b)\geq 1, \quad i=1, \ldots ,m. \]

Portanto, o problema de encontrar o hiperplano ótimo pode ser formulado da seguinte maneira
\[ \label{eq:problemageralSVM}
\begin{aligned}
\min_{w,b} & \quad \dfrac{1}{2} \Vert w\Vert^{2} \\
\text{s.a.} &  \quad y_i(w^{T}x^{i}+b) \geq 1, \quad i=1, \ldots , m, \end{aligned}
 \]
em que $w\in \RR^{n}$ e $b\in \RR$. 

O problema \eqref{eq:problemageralSVM} possui função objetivo 
\[ f(w,b)=\dfrac{1}{2}\Vert w\Vert^{2} \]
convexa, e restrições lineares
\[ g_i(w,b)=1-y_i(w^{T}x^{i}+b) \leq 0, \quad i=1, \ldots, m, \]
em que a função $g:\RR^{n+1} \rightarrow \RR^{m}$ pode ser escrita da forma 
\[ g(w,b)= e - (YX^{T}w+by) \leq 0, \]
com $e$ sendo o vetor cujas $m$ componentes são todas iguais a $1$, $Y=\diag(y_{i})$, $X=\diag(x^{i})$, $y^{T}=[y_{1} \ \ldots \ y_{m}]$, $w\in \RR^n$ e $b\in \RR$.


%----------------------------------------------------------------------------------------------------------------------

\subsection{Formulação Matemática do Problema de Classificação - Margem Flexível (CSVM)}

Situações reias dificilmente envolvem problemas cujos dados são linearmente separáveis. Em vista disso, faz-se necessário estender os conceitos e resultados estudados nas SVMs lineares de margem rígida para o caso de SVM com margem flexível, quando os dados não são linearmente separáveis. Para tanto, considere um conjunto de dados não linearmente sepavável como da \Cref{fig4:b}, isto é, não existe um hiperplano separador. 


\begin{figure}[htbp] 
	\centering
	\begin{subfigure}[h]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{dados_linearmenteseparaveismmll}
		\caption{Dados linearmente separáveis. \label{fig4:a}}
	\end{subfigure}
	\begin{subfigure}[h]{0.31\textwidth}
		\centering
		\includegraphics[width=\textwidth]{dados_nlinearmenteseparaveismml}
		\caption{Dados não linearmente separáveis. \label{fig4:b}}
	\end{subfigure}
\caption{Fonte: \textcite{Faisal2019} \label{fig4}}
\end{figure}


Neste caso, temos que o conjunto viável
\[
\{ (w,b) \in \RR^{n+1} \mid 1-y_{i}(w^{T}x^{i} + b) \leq 0 , \quad i=1, \ldots , m \}
\]
é vazio e, portanto, a formulação dada pelo problema \eqref{eq:problemageralSVM} não fornece um classificador.

Assim, no intuito de contornar esse problema utilizamos regularização para suavizar as margens, acrescentando variáveis de folga $\xi_{i} \geq 0$ associadas aos dados de treinamento $x_{i}$, com $i=1, \ldots , m$, e permitindo, assim, uma flexibilização do problema de estimar as variáveis $w$ e $b$. Em outras palavras, a restrição $1-y_{i}(w^{T}x^{i} + b) \leq 0$ é relaxada e substituída por $1-y_{i}(w^{T}x^{i} + b) \leq \xi_{i} $, com $\xi_{i} \geq 0$. Cada variável de folga $\xi_{i}$ mensura a distância que determinado dado $x_{i}$ está do seu respectivo hiperplano separador, caso este dado esteja do lado errado.

%Esta imagem também não está sendo lida, detectar problema #chateada -Uhhhul deu certo! LEMBRETE: não usar acento ao nomear figuras
\begin{figure}[!h] 
	\centering
	\includegraphics[width=0.40\textwidth]{mmlinterpretacao_variavel_xi}
	\caption{Variáveis de folga \label{fig5} \\ Fonte: \textcite{Faisal2019}}
\end{figure}

Tal procedimento permite que pontos da classe positiva permaneçam fora do semiespaço $\mathcal{S}^{+}=\{x\in \RR^n \mid w^{T}x+b\geq 1\}$ e/ou pontos da classe negativa fora do semiespaço $\mathcal{S}^{-}=\{x\in \RR^n \mid w^{T}x+b\leq -1\}$. 

Nesta formulação o hiperplano separador é denominado hiperplano de margem flexível e as restrições dos hiperplanos separadores são reformuladas da seguinte maneira 
\begin{align}
w^{T}x^{i}+b &\geq 1 - \xi_{i} , \text{ para  todo } x^{i} \in \Xset^{+}, \label{restricoesCSVM+} \\
w^{T}x^{i}+b &\leq -1 +\xi_{i} , \text{ para  todo } x^{i} \in \Xset^{-}. \label{restricoesCSVM-}
\end{align}


Agora nosso objetivo é encontrar $w$ e $b$ ótimos de modo a obter um bom classificador. Primeiramente, observe que dados $w$ e $b$ arbitrários, podemos escolher $\xi_{i} \geq 0$ de modo que as restrições \eqref{restricoesCSVM+} e \eqref{restricoesCSVM-} sejam satisfeitas. Para tanto, podemos definir
\[
\xi_{i} =  \left \{ \begin{array}{cc} \max\{0, 1-w^{T}x^{i}-b\}, & \quad \text{se} \quad x^{i} \in \Xset^{+}, \\
\max\{0, 1+w^{T}x^{i}+b\}, & \quad \text{se} \quad x^{i} \in \Xset^{-}.
\end{array} \right .
\]
 
Desse modo, para obter um bom classificador não basta apenas maximizar a margem definida pelos hiperplanos $\Hset^{+}$ e $\Hset^{-}$ e introduzir as variáveis de folga nas restrições, mantendo a mesma função objetivo, pois, como exemplificado por \textcite{Evelin2017}, a \Cref{fig6} ilustra o hiperplano dado por $w_{0}^{T}x+b = 0$, que não poder ser usado para classificar os dados, mas satisfaz as restrições \eqref{restricoesCSVM+} e \eqref{restricoesCSVM-}. 

\begin{figure}[!h] 
	\centering
	\includegraphics[width=0.40\textwidth]{restricoes_maximizacaomargem}
	\caption{Pensar. \label{fig6} \\ Fonte: \textcite{Evelin2017}}
\end{figure}
 

Portanto, para reformular o problema original de maximizar a margem é preciso também controlar o valor dessas variáveis de modo a estimular uma classificação correta, pois quanto maior o valor delas mais será permitido violar as restrições. Em vista disso, é acrescentada na função objetivo uma parcela que corresponde à penalização das violações e o problema \eqref{eq:problemageralSVM} é reformulado da seguinte forma

\[ \label{eq:problemaCSVM}
\begin{aligned}
\min_{w,b,\xi} & \quad \dfrac{1}{2} \Vert w\Vert^{2} + C \sum_{i=1}^{m} \xi_{i} \\
\text{s.a.} &  \quad y_i(w^{T}x^{i}+b) \geq 1 - \xi_{i}, \quad i=1, \ldots , m, \\
& \xi_{i} \geq 0, \quad i=1, \ldots , m,\end{aligned}
\]
%não consegui alinhar a última linha às duas primeiras
em que $C>0$ é um parâmetro de regularização que tem o objetivo de controlar a importância das variáveis de folga. O valor do parâmetro $C$ que fornece uma boa classificação dos dados é escolhido de maneira heurística na fase de treinamento, geralmente a partir da natureza do problema.É devido a utilização desse parâmetro esta modelagem de SVM também é conhecida como C-SVM.

O termo $C \sum_{i=1}^{m} \xi_{i}$ na função objetivo do problema \eqref{eq:problemaCSVM} pode ser pensado como uma medida de erro de classificação, pois minimiza o valor das variáveis de folga e reduz desse modo o número de pontos classificados incorretamente. De fato, aumentando o valor do parâmetro $C$ aumenta-se a penalização sobre a violação da restrição original do problema SVM. Por outro lado, diminuindo o valor de $C$ o modelo se torna mais flexível a esse tipo de violação. 

O problema de margem flexível \eqref{eq:problemaCSVM}, assim como o problema \eqref{eq:problemageralSVM}, também possui restrições lineares 
\begin{align} 
g_{i}(w,b,\xi) = & 1-\xi_{i} - y_i(w^{T}x^{i}+b) \leq 0 \quad \text{e} \\
h_{i}(w,b,\xi) = & - \xi_{i} \leq 0, \quad i=1, \ldots, m,
\end{align}

e assim, o conjunto viável
\[
\Omega = \{(w,b,\xi) \in \RR^{n+1+m} \mid g_{i}(w,b,\xi) \leq 0, \, h_{i}(w,b,\xi) \leq 0, \, i=1, \ldots, m \} 
\]
é um poliedro não vazio.

Ademais, a função objetivo $f$ é quadrática e limitada inferiormente em $\Omega$, pois 
\[
f(w,b,\xi) = \dfrac{1}{2} \Vert w\Vert^{2} + \underbrace{C}_{> 0} \sum_{i=1}^{m} \underbrace{\xi_{i}}_{\geq 0} \geq 0.
\]
%Assim o Teorema de Programação Quadrática (1.34) garante a existência de um minimizador global para o problema \eqref{eq:problemaCSVM}.

%-------------------------------------------------------------------------------------------------------------------------


\section{Conceitos de Otimização}

Nesta seção abordaremos alguns conceitos e resultados de Otimização para o problema
\[ \label{problema_geral_otimizacao}
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad g(x) \leq 0 , \\
& \quad h(x) = 0,
\end{aligned}
\]
em que a função objetivo $f: \RR^{n} \rightarrow \RR$ e as funções que definem as restrições $g: \RR^{n} \rightarrow \RR^{m}$ e $h:\RR^{n} \rightarrow \RR^{p}$ são continuamente diferenciáveis. O conjunto
\[
\Omega := \{ x \in \RR^{n} \mid g(x)\leq 0 , h(x)=0 \}
\]
é chamado de \emph{conjunto viável} do problema \eqref{problema_geral_otimizacao}, e os pontos de $\Omega$ são chamados de \emph{pontos viáveis}.


\begin{defi} \label{defi:definicao_minimizador}
Dizemos que $x^{*}$ é uma solução global do problema \eqref{problema_geral_otimizacao}, ou seja, um \emph{minimizador global} de $f$ em $\Omega$, quando
\[
f(x^{*}) \leq f(x)
\]
para todo $x \in \Omega $. 
\end{defi}

Se a função objetivo $f$ for linear, \eqref{problema_geral_otimizacao} é chamado de problema de \emph{programação linear}, e se $f$ for uma função quadrática, \eqref{problema_geral_otimizacao} é chamado de problema de \emph{programação quadrática}. Ademais, quando $\Omega$ é um conjunto convexo e $f$ é uma função convexa, dizemos que \eqref{problema_geral_otimizacao} é um problema de \emph{programação convexa}. 

%Quando as desigualdades na \Cref{defi:definicao_minimizador} forem estritas para $x \neq x^{*}$, dizemos que $x^{*}$ é minimizador estrito. Quando $\Omega = \RR^{n}$ dizemos que o problema \eqref{problema_geral_otimizacao} é \emph{irrestrito}, e quando $\Omega \neq \RR^{n}$ falamos de \emph{otimização com restrições}.


\begin{defi}
Dizemos que $x^{*} \in [-\infty, +\infty )$ definido por
\[
x^{*} = \inf_{x \in \Omega} f(x)
\]
é o \emph{valor ótimo} do problema \eqref{problema_geral_otimizacao}.
\end{defi}


%Uma função $f:\RR^{n} \rightarrow \RR$ definida por
%\[
%f(x) = x^{T}Qx + c^{T}x ,
%\]
%em que $Q \in \RR^{nxn}$, $c \in \RR^{n}$, é chamada \emph{função quadrática}.

%Veremos mais a frente que no caso em que $Q$ é uma matriz semidefinida positiva, os problemas quadráticos são convexos.

\begin{defi}
Dado $\xbar \in \Omega$, dizemos que uma restrição de desigualdade $g_{i}$ é ativa em $\xbar$ quando $g_{i}(\xbar)=0$. Caso $g_{i}(\xbar)<0$, dizemos que $g_{i}$ é inativa em $\xbar$. 

O conjunto dos índices das restrições de desigualdade ativas é denotado por
\[
I(\xbar) = \{ i \mid g_{i}(\xbar)=0 \} .
\]
\end{defi}


\begin{defi} 
Um ponto $x^{*} \in \RR^{n} $ é dito \emph{estacionário} para o problema \eqref{problema_geral_otimizacao} quando existirem $\mu^{*} \in \RR^{m} $, $\lambda^{*} \in \RR^{p} $ (\emph{multiplicadores de Lagrange}) tais que

\begin{align} \label{eq:1_ponto_estacionario}
\nabla f(x^{*})+\sum_{i=1}^{m} \mu_{i}^{*} \nabla g_{i}(x^{*})+\sum_{i=1}^{p} \lambda_{i}^{*} \nabla h_{i}(x^{*}) & =0, \\ 
g(x^{*}) \leq 0, h(x^{*}) & =0,  \label{eq:2_ponto_estacionario} \\ 
\mu_{i}^{*} & \geq 0, \quad i= 1, \ldots , m,   \label{eq:3_ponto_estacionario} \\ 
\lambda_{i}^{*} g_{i}(x^{*}) & =0, \quad i= 1, \ldots , m.  \label{eq:4_ponto_estacionario}
\end{align}


\end{defi}
%ver como enumerar cada condição separadamente, pois não está aceitando
As condições \eqref{eq:1_ponto_estacionario} são conhecidas como condições de Karush-Kuhn-Tucker (KKT). Assim, um ponto é dito estacionário quando satisfaz as condições KKT. Sob determinadas hipóteses de qualificação, as condições KKT são satisfeitas em um ponto que seja minimizador.

\begin{defi}
Dizemos que as restrições $g(x) \leq 0$ e $h(x)=0$ cumprem uma condição de qualificação em $x^{*} \in \Omega$ quando, dada qualquer função diferenciável $f$ que tenha mínimo em $x^{*}$, relativamente a $\Omega$, sejam satisfeitas as condições de otimalidade de KKT.
\end{defi}
Logo, um ponto $x \in \RR^{n}$ é dito qualificado quando atende uma condição de qualificação. Vejamos então algumas condições de qualificação para o problema \eqref{problema_geral_otimizacao}.

\textbf{Condição de qualificação de Slater:} Dizemos que a condição de qualificação de Slater é satisfeita quando $h$ é linear, cada componente $g_{i}$, $i=1, \ldots , m$, é convexa e existe $\tilde{x} \in \Omega$ tal que $h(\tilde{x})=0$ e $g(\tilde{x})<0$.

\textbf{Condição de qualificação de independência linear:} Dizemos que a condição de qualificação de independência linear (LICQ) é satisfeita em $\xbar$ quando o conjunto formado pelos gradientes das restrições de igualdade e das restrições de desigualdade ativas é linearmente independente, isto é, 
\[
\{ \nabla g_{i}(\xbar) \mid i \in I(\xbar) \} \cup \{ \nabla h_{i}(\xbar), i=1, \ldots , p \} \quad \text{é LI}.
\] 

\begin{teo}(\emph{KKT}) Seja $x^{*} \in \RR^{n}$ um minimizador local do problema \eqref{problema_geral_otimizacao} e suponha que seja satisfeita uma condição de qualificação. Então existem vetores $\mu^{*} \in \RR^{m}$ e $\lambda^{*} \in \RR^{p}$ tais que $(x^{*} , \mu^{*} , \lambda^{*} )$ cumpre \eqref{eq:1_ponto_estacionario}.
\end{teo}

É importante observar que se não for verificada nenhuma condição de qualificação para as restrições, podemos ter minimizadores que não cumprem KKT, o que dificulta a caracterização de tais pontos. Vejamos a seguir um exemplo que justifica essa afirmação.

\begin{exem}
Considere o problema 
\[
\begin{aligned}
\min_{x} & \quad f(x)=x_{1} \\
\text{s.a} & \quad g_{1}(x)=-x_{1}^{3} + x_{2} \leq 0 , \\
& \quad g_{2}(x) = -x_{2} \leq 0.
\end{aligned}
\]

O ponto $x^{*} = 0$ é o minimizador deste problema, mas não cumpre as condições KKT. De fato, observe que $0\leq x_{2} \leq x_{1}^{3}$, o que implica em $f(x)= x_{1} \geq 0 = f(x^{*})$, para todo ponto viável $x$.

No entanto, 
\[
\nabla f(x^{*}) = \left( \begin{array}{cc} 1 \\ 0 \end{array} \right) , \nabla g_{1}(x^{*}) = \left( \begin{array}{cc} 0 \\ 1 \end{array} \right) \quad \text{e} \quad \nabla g_{2}(x^{*}) = \left( \begin{array}{cc} 0 \\ -1 \end{array} \right) ,
\]
ou seja, os gradientes das restrições não são LI e, portanto, não satisfazem a condição de qualificação de independência linear (LICQ). Assim, as condições KKT não são satisfeitas, pois não existem $\mu_{1}^{*} , \mu_{2}^{*} \in \RR^{+}$ tais que 
\[
\left( \begin{array}{cc} 1 \\ 0 \end{array} \right) + \mu_{1}^{*} \left( \begin{array}{cc} 0 \\ 1 \end{array} \right) + \mu_{2}^{*} \left( \begin{array}{cc} 0 \\ -1 \end{array} \right) = \left( \begin{array}{cc} 0 \\ 0 \end{array} \right) .
\]
\end{exem}


Uma classe especial de problemas de otimização se refere ao caso em que o conjunto viável $\Omega$ é um conjunto poliedral.

\begin{defi}(\textbf{Conjunto viável poliedral:}) Um conjunto viável é \emph{poliedral} quando as restrições $g$ e $h$ são afins. Ou seja, o problema terá o seguinte formato
\[
\begin{aligned} \label{eq:conjunto_viavel_poliedral}
\min_{x} & \quad f(x)  \\
\text{s.a} & \quad Ax \leq b  \\
& \quad Mx = r.
\end{aligned}
\]
em que $A \in \RR^{mxn}$, $M\in \RR^{pxn}$, $b \in \RR^{m}$, $r \in \RR^{p}$. 
%Neste contexto, dizemos que $g:\RR^{n} \rightarrow \RR^{m}$, com $g(x) = Ax-b$, e $h:\RR^{n}\rightarrow \RR^{p}$, com $h(x) = Mx - r$, são funções afins.
\end{defi}

Neste caso, todo ponto viável é qualificado. De fato (DEMONSTRAR).

Portanto, se $x^{*}$ é um minimizador local do problema \eqref{eq:conjunto_viavel_poliedral}, então $x^{*}$ satisfaz as condições KKT.















%-------------------------------------------------------------------------------------------------------------------------


\section{Otimização Convexa}

Em otimização uma hipótese com ótimas consequências é a convexidade, pois ela garante que pontos estacionários são minimizadores e permite concluir que minimizadores locais são globais. A partir disso, os problemas de classificação podem ser formulados em termos de otimização convexa.
Os principais conceitos definidos neste capítulo são conjuntos convexos e funções convexas e a partir deles apresentamos alguns resultados importantes da análise convexa. Para desenvolvimento desse capítulo as principais referências utilizadas foram \textcite{Evelin2017,Ademir2013,Izmailov2014ac}.


\subsection{Conjuntos Convexos}

\begin{defi} 
Um conjunto $C \subset \RR^{n}$ é dito convexo quando dados $x,y \in C$, o segmento $[x,y] = \{ (1-t)x + ty \mid t\in [0,1] \}$ estiver inteiramente contido em $C$.
\end{defi}

A \Cref{fig:conjuntos_convexos} apresenta a noção de convexidade de conjuntos, ilustrando um conjunto convexo e outro não convexo. Em outras palavras um conjunto convexo se caracteriza por conter todos os segmentos cujos extremos pertencem ao conjunto, o que não ocorre no segundo conjunto da \Cref{fig:conjuntos_convexos} por exemplo.


%criar minha própria figura
\begin{figure}[!h] 
	\centering
	\includegraphics[width=0.40\textwidth]{conjunto_convexo}
	\caption{ Exemplo de conjunto convexo e não convexo. \label{fig:conjuntos_convexos} \\ Fonte: \textcite{Evelin2017}}
\end{figure}


Alguns exemplos de conjuntos convexos são o conjunto vazio, o espaço $\RR^{n}$, qualquer hiperplano do $\RR^{n}$ e um conjunto que contém um ponto só.

A seguir, apresentamos alguns resultados importantes da análise convexa.

%{\cite[p. 50]{Ademir2013}}
\begin{lema} \label{lema:1_convexidade}
Considere $\Vert \cdot \Vert$ a norma euclidiana em $\RR^{n}$. Sejam $u,v \in \RR^{n}$ com $u\neq v$. Se $\Vert u \Vert = \Vert v \Vert = r$, então $\Vert (1-t)u + tv \Vert < r$, para todo $t \in (0,1)$.
\end{lema}

\begin{proof}
Seja $t \in (0,1)$ e suponha que $\Vert u \Vert = \Vert v \Vert = r$. Aplicando a desigualdade triangular, temos
\[
\Vert (1-t)u + tv \Vert \leq (1-t)\Vert u \Vert + t\Vert v \Vert = (1-t)r + tr = r.
\]

Agora, suponha por absurdo que $\Vert (1-t)u + tv \Vert = r$. Então
\[ \label{eq:1_lema1_convexidade}
(1-t)^{2} u^{T}u + 2t(1-t)u^{T}v + t^{2}v^{T}v = \Vert (1-t)u + tv \Vert^{2} = r^{2}.
\]

Como $u^{T}u = v^{T}v = r^{2}$ e $t \in (0,1)$, substituindo em \eqref{eq:1_lema1_convexidade} e desenvolvendo, obtemos 
\begin{align}
r^{2} &= (1-2t+t^{2}) u^{T}u + (2t-2t^{2})u^{T}v + t^{2}v^{T}v \\
&= (1-2t+t^{2}) r^{2} + (2t-2t^{2})u^{T}v + t^{2}r^{2} \\
&= r^{2} - 2tr^{2} + t^{2}r^{2} + (2t-2t^{2})u^{T}v + t^{2}r^{2} .
\end{align}

Evidenciando $r^{2}$, temos
\[ 
(2t-2t^{2})r^{2} = (2t-2t^{2})u^{T}v,
\]
e, portanto,
\[ \label{eq:2_lema1_convexidade}
r^{2} = u^{T}v.
\]

Assim, por \eqref{eq:2_lema1_convexidade},
\[
\Vert u-v \Vert^{2} = u^{T}u - 2u^{T}v + v^{T}v = r^{2} - 2r^{2} + r^{2} = 0,
\]
o que é uma contradição, pois por hipótese $u \neq v$. 

Portanto, concluímos que $\Vert (1-t)u + tv \Vert < r$, para todo $t \in (0,1)$.
\end{proof}

Agora, dado um conjunto $S \subset \RR^{n}$ e um ponto $z \in \RR^{n}$, considere o problema de encontrar um ponto de $S$ mais próximo de $z$, em outras palavras, queremos minimizar a distânica de um ponto a um conjunto. Assim, os próximos resultados garantem a existência da solução no caso de $S$ ser um conjunto fechado e sua unicidade se, além de fechado, $S$ for convexo. Tal solução é chamada de projeção de $z$ sobre $S$, e denotada por $\proj_{S} (z)$. 

\begin{lema} \label{lema:existencia_projecao}
Seja $S \subset \RR^{n}$ um conjunto fechado não vazio. Dado $z \in \RR^{n}$, existe $\bar{z} \in S$ tal que
\[
\Vert z - \bar{z} \Vert \leq \Vert z - x \Vert,
\]
para todo $x \in S$.
\end{lema}
\begin{proof}
Seja $\alpha = \inf \{\Vert z-x \Vert \mid x \in S\}$. Então, para cada $n \in \mathds{N}$, existe $x^{n} \in S$ tal que
\[ \label{eq:1_lema_existencia_projecao}
\alpha \leq \Vert z-x^{n} \Vert \leq \alpha + \dfrac{1}{n}. 
\]

Em particular, $\Vert z-x^{n} \Vert \leq \alpha + 1$, para todo $n \in \mathds{N}$. Logo, existe uma subsequência $(x^{n^{k}})$ convergente, com $k \in \mathds{N}'$, tal que $x^{n^{k}} \longrightarrow \bar{z}$. Como $S$ é fechado temos que $\bar{z} \in S$. Além disso, 
\[
\Vert z-x^{n} \Vert \longrightarrow \Vert z-\bar{z} \Vert .
\]

Mas, por \eqref{eq:1_lema_existencia_projecao}, temos que $\Vert z-x^{n} \Vert \longrightarrow \alpha $, e portanto, concluímos que $\Vert z-\bar{z} \Vert = \alpha$.

\end{proof}


\begin{lema}  \label{lema:unicidadeprojecao_convexidade}
Seja $S \subset \RR^{n}$ um conjunto não vazio, convexo e fechado. Dado $z \in \RR^{n}$, existe um único $\bar{z} \in S$ tal que
\[
\Vert z - \bar{z} \Vert \leq \Vert z - x \Vert
\]
para todo $x \in S$.
\end{lema}
\begin{proof}
A existência é garantida pelo \Cref{lema:existencia_projecao}. Para provar a unicidade suponha que existam $\bar{z}, \tilde{z} \in S$, com $\bar{z} \neq \tilde{z}$, ta{}is que
\[ \label{eq:1_lema_unicidade_projecao}
\Vert z-\bar{z} \Vert \leq \Vert z-x \Vert \quad \text{e} \quad \Vert z-\tilde{z} \Vert \leq \Vert z-x \Vert ,
\]
para todo $x \in S$. Tomando $x=\tilde{z}$ na primeira desigualdade e $x=\bar{z}$ na segunda, obtemos
\[
\Vert z-\bar{z} \Vert = \Vert z-\tilde{z} \Vert .
\]

Por outro lado, o ponto $z^{*} = \dfrac{\bar{z} - \tilde{z}}{2}$ pertence ao conjunto convexo $S$. Além disso, pelo \Cref{lema:1_convexidade}, com $r= \Vert z-\bar{z} \Vert = \Vert z - \tilde{z} \Vert$ e $t=1/2$, temos
\begin{align}
\Vert z-z^{*} \Vert &=  \Vert z-t(\bar{z}+\tilde{z}) \Vert \\
& = \Vert z - t\bar{z} - t\tilde{z} \Vert \\
& = \Vert (1-t)(z-\bar{z}) + t(z-\tilde{z}) \Vert \\
& < r ,
\end{align}
o que é uma contradição, pois por \eqref{eq:1_lema_unicidade_projecao} teríamos
\[
r = \Vert z-\bar{z} \Vert = \Vert z - \tilde{z} \Vert \leq \Vert z-z^{*} \Vert < r .
\]

Portanto, $\bar{z} = \tilde{z}$.


\end{proof}


No \Cref{lema:unicidadeprojecao_convexidade} denotamos $\bar{z} = \proj_{S} (z)$.


\begin{teo}  \label{lema_condicaoprojecao_convexidade}
Sejam $S \subset \RR^{n}$ um conjunto não vazio, convexo e fechado, $z \in \RR^{n}$ e $\bar{z} = \proj_{S} (z)$. Então, 
\[
(z - \bar{z})^{T}(x - \bar{z}) \leq 0 ,
\]
para todo $x \in S$.
\end{teo}
\begin{proof}
Sejam $x \in S$ um ponto arbitrário e $\bar{z} = \proj_{S} (z)$. Pelo \Cref{lema:existencia_projecao} $\bar{z} \in S$ e, dado $t \in (0,1)$, pela convexidade de $S$, temos que $(1-t)\bar{z} + tx \in S$. Assim, 
\[
\Vert z-\bar{z} \Vert \leq \Vert z-(1-t)\bar{z} - tx \Vert = \Vert (z-\bar{z}) - t(x-\bar{z}) \Vert.
\]

Então, 
\[
\Vert z-\bar{z} \Vert^{2} \leq \Vert (z-\bar{z}) - t(x-\bar{z}) \Vert^{2} = \Vert z-\bar{z} \Vert^{2} - 2t(z-\bar{z})^{T}(x-\bar{z}) + t^{2}\Vert x- \bar{z} \Vert^{2} ,
\]
e como $t>0$, temos
\[ \label{eq:1_lema_condicaoprojecao_convexidade}
2(z-\bar{z})^{T}(x-\bar{z}) \leq t \Vert x- \bar{z} \Vert^{2} .
\]

Passando o limite em \eqref{eq:1_lema_condicaoprojecao_convexidade} quando $t\rightarrow 0$, obtemos
\[
(z-\bar{z})^{T}(x-\bar{z}) \leq 0, 
\]
completando a demonstração.
\end{proof}


O \Cref{lema_condicaoprojecao_convexidade} estabelece uma condição necessária e suficiente para caracterizar a projeção. Este resultado é provado no lema seguinte.


\begin{lema} \label{lema:defineprojecao_convexidade}
Sejam $S \subset \RR^{n}$ um conjunto não vazio, convexo e fechado e $z \in \RR^{n}$. Se $\bar{z} \in S$ satisfaz
\[
(z - \bar{z})^{T}(x - \bar{z}) \leq 0 ,
\]
para todo $x \in S$, então $\bar{z} = \proj_{S} (z)$.
\end{lema}
\begin{proof}
Dado $x\in S$ arbitrário, temos
\begin{align}
\Vert z-\bar{z} \Vert^{2} - \Vert z-x \Vert^{2} & = z^{T}z - 2z^{T}\bar{z} + \bar{z}^{T}\bar{z} - z^{T}z + 2z^{T}x - x^{T}x \\
& = - 2z^{T}\bar{z} + \bar{z}^{T}\bar{z} + 2z^{T}x - x^{T}x \\
& = (x-\bar{z})^{T}(2z-x-\bar{z}) \\
& = (x-\bar{z})^{T}(2(z-\bar{z})-(x-\bar{z})) \\
& = 2(x-\bar{z})^{T}(z-\bar{z}) - (x-\bar{z})^{T}(x-\bar{z}) \\
& \leq 0 .
\end{align}
pois $(x-\bar{z})^{T}(z-\bar{z}) \leq 0$ por hipótese, e $(x-\bar{z})^{T}(x-\bar{z}) = \Vert (x-\bar{z}) \Vert \geq 0$.

Logo,
\[
\Vert z-\bar{z} \Vert^{2} - \Vert z-x \Vert^{2} \leq 0,
\]
e, então
\[
\Vert z-\bar{z} \Vert^{2} \leq \Vert z-x \Vert^{2} ,
\]
para todo $x\in S$.

Portanto, $\bar{z} = \proj_{S} (z)$.
\end{proof}


O \Cref{lema:defineprojecao_convexidade} fornece uma condição necessária de otimalidade ao minimizar uma função em um conjunto convexo fechado.

\begin{teo}(\textbf{Taylor de Primeira Ordem}) {\cite[p.25]{Ademir2013}} \label{teo:Taylor_primeira_ordem}
Considere $f:\RR^{n} \rightarrow \RR$ uma função diferenciável e $\xbar \in \RR^{n}$. Então podemos escrever
\[
f(x) = f(\xbar) + \nabla f(\xbar)^{T}(x-\xbar) + r(x),
\]
com $\lim_{x \rightarrow \xbar}\dfrac{r(x)}{\Vert x-\xbar \Vert} = 0$.
\end{teo}


\begin{lema} \label{lema:associa_projecao_minimizador}
Sejam $f: \RR^{n} \rightarrow \RR$ uma função diferenciável e $C \subset \RR^{n}$ um conjunto convexo e fechado. Se $x^{*} \in C$ é minimizador local de $f$ em $C$, então
\[
\proj_{C} (x^{*} - \alpha \nabla f(x^{*})) = x^{*} ,
\]
para todo $\alpha \geq 0$.
\end{lema}
\begin{proof}
Seja $x^{*} \in C$ minimizador local de $f$ em $C$. Fixando $x\in C$, como $C$ é um conjunto convexo, temos
\[ \label{eq:1_lema_associa_projecao_minimizador}
f(x^{*}) \leq f((1-t)x^{*} + tx),
\]
para todo $t\geq 0$ suficientemente pequeno. Pelo \Cref{teo:Taylor_primeira_ordem}, 
\[ \label{eq:2_lema_associa_projecao_minimizador}
f(x^{*}+t(x-x^{*})) = f(x^{*}) + t\nabla f(x^{*})^{T}(x-x^{*}) + r(t) ,
\]
em que $\lim_{t\rightarrow 0} \dfrac{r(t)}{t} = 0$. Dessa forma, por \eqref{eq:1_lema_associa_projecao_minimizador} e \eqref{eq:2_lema_associa_projecao_minimizador}, temos
\[
0 \leq f(x^{*}+t(x-x^{*})) - f(x^{*}) = t\nabla f(x^{*})^{T}(x-x^{*}) + r(t) ,
\]
e portanto, 
\[ \label{eq:3_lema_associa_projecao_minimizador}
t\nabla f(x^{*})^{T}(x-x^{*}) + r(t) \geq 0.
\]

Dividindo por $t$ e passando o limite quando $t \rightarrow 0$ em \eqref{eq:3_lema_associa_projecao_minimizador}, obtemos
\[
\nabla f(x^{*})^{T}(x-x^{*}) \geq 0.
\]

Assim, dado $\alpha \geq 0$,
\[
(x^{*} - \alpha \nabla f(x^{*}) - x^{*})^{T}(x-x^{*}) = -\alpha \nabla f(x^{*})^{T}(x-x^{*}) \leq 0,
\]
para todo $x\in C$.

Portanto, pelo \Cref{lema:defineprojecao_convexidade} concluímos que $\proj_{C} (x^{*} - \alpha \nabla f(x^{*})) = x^{*}$, para todo $\alpha \geq 0$.
\end{proof}


%----------------------------------------------------------------------------------------------------------------------------


\subsection{Funções Convexas}

\begin{defi} \label{defi:funcao_convexa}
Seja $C \subset \RR^{n}$ um conjunto convexo. Dizemos que a função $f: \RR^{n} \rightarrow \RR$ é convexa em $C$ quando
\[
f((1-t)x + ty) \leq (1-t)f(x) + tf(y),
\]
para todos $x,y \in C$ e $t \in [0,1]$.
\end{defi}
Se para todo $t \in (0,1)$ e $x \neq y$ vale que
\[
f((1-t)x + ty) < (1-t)f(x) + tf(y),
\]
dizemos que $f$ é estritamente convexa.

A noção geométrica da \Cref{defi:funcao_convexa} é apresentada na \Cref{fig_funcao_convexa}, em que qualquer arco do gráfico de uma função convexa está sempre abaixo do segmento que liga as extremidades.


%criar minha própria imagem
\begin{figure}[!h] 
	\centering
	\includegraphics[width=0.85\textwidth]{funcao_convexa}
	\caption{ Função Convexa. \label{fig_funcao_convexa} \\ Fonte: \textcite{Evelin2017}}
\end{figure}


\begin{obs}
Todo problema de maximização
\[
\begin{aligned}
\max_{x} & \quad f(x) \\
\text{s.a.} & \quad x \in \Omega \end{aligned}
 \]
pode ser transformado em um problema de minimização equivalente
\[
\begin{aligned}
\min_{x} & \quad -f(x) \\
\text{s.a.} & \quad x \in \Omega .\end{aligned}
 \]
\end{obs}


\begin{defi}
Se $C \subset \RR^{n}$ é um conjunto convexo, dizemos que $f: C \rightarrow \RR $ é uma \emph{função côncava} em $C$, quando a função $(-f)$ é convexa em $C$.
\end{defi}


Assim, maximizar uma função côncava num conjunto convexo equivale a minimizar uma função convexa num conjunto convexo.


O teorema seguinte justifica o fato da convexidade ser uma propriedade tão importante em otimização.


%{\cite[p. 57]{Ademir2013}}
\begin{teo} 
Sejam $C \subset \RR^{n}$ um conjunto convexo e $f:C \rightarrow \RR$ uma função convexa. Se $x^{*} \in C$ é minimizador local de $f$, então $x^{*}$ é minimizador global de $f$.
\end{teo}
\begin{proof}
Seja $x^{*}$ um minimizador local de $f$. Então, existe $\delta > 0$ tal que 
\[
f(x^{*}) \leq f(x), 
\]
para todo $x \in B(x^{*}, \delta) \cap C$. 

Considere $y \in C$, tal que $y \notin B(x^{*}, \delta)$, e tome $t \in (0,1]$ de modo que $t \Vert y-x^{*} \Vert < \delta$. Assim, o ponto $x = (1-t)x^{*} + ty$ satisfaz
\[
\Vert x - x^{*} \Vert = \Vert (1-t)x^{*} + ty - x^{*} \Vert = \Vert x^{*} - tx^{*} + ty - x^{*} \Vert = t \Vert y - x^{*} \Vert < \delta ,
\]
e, portanto, $x \in B(x^{*}, \delta) \cap C$.

Desse modo, como $f$ é uma função convexa, temos
\[
f(x^{*}) \leq f(x) \leq (1-t)f(x^{*}) + tf(y) = f(x^{*}) + t(f(y)-f(x^{*})),
\]
donde segue que $f(x^{*}) \leq f(y)$.

Portanto, $x^{*}$ é minimizador global de $f$.
\end{proof}


A seguir apresentamos outra forma de caracterizar a convexidade de uma função quando temos hipóteses de diferenciabilidade.


%{\cite[p. 58]{Ademir2013}}
\begin{teo}  \label{teo:diferenciabilidade_e_convexidade}
Sejam $f: \RR^{n} \rightarrow \RR $ uma função diferenciável e $C \in \RR^{n}$ um conjunto convexo. A função $f$ é convexa em $C$ se, e somente se, 
\[
f(y) \geq f(x) + \nabla f(x)^{T}(y-x),
\]
para todos $x, y \in C$.
\end{teo}


\begin{proof}
Seja $f$ uma função convexa. Para $x,y \in C$ e $t \in (0,1]$ quaisquer, temos $(1-t)x + ty = x + t(y-x)$. Assim, definindo $d = y-x$, temos que $x + td \in C$ e
\[
f(x+td) = f((1-t)x + ty) \leq (1-t)f(x) + tf(y),
\]
logo
\[
f(x+td) \leq f(x) + t(f(y)-f(x)),
\]
o que implica em 
\[
f(y)-f(x) \geq \dfrac{f(x+td)-f(x)}{t} .
\]

Passando o limite quando $t \rightarrow 0^{+}$, temos
\[
f(y)-f(x) \geq \lim_{t \rightarrow 0^{+}} \dfrac{f(x+td)-f(x)}{t} = \nabla f(x)^{T} d = \nabla f(x)^{T} (y-x) ,
\]
e, portanto,
\[
f(y) \geq f(x) + \nabla f(x)^{T} (y-x) .
\]

Reciprocamente, considere $z = (1-t)x + ty$ e observe que 
\[
f(x) \geq f(z) + \nabla f(z)^{T}(x-z) \quad \text{e} \quad f(y) \geq f(z) + \nabla f(z)^{T}(y-z) .
\]

Agora, multiplicando a primeira desigualdade por $(1-t)$ e a segunda por $t$, obtemos
\begin{align}
(1-t)f(x) + tf(y) & \geq (1-t)(f(z) + \nabla f(z)^{T}(x-z)) + t(f(z) + \nabla f(z)^{T}(y-z)) \\
& \geq f(z) + \nabla f(z)^{T}(x-z) - tf(z) -t\nabla f(z)^{T}(x-z) + tf(z) + t\nabla f(z)^{T}(y-z) \\
& \geq f(z) + \nabla f(z)^{T}(x-z) + t\nabla f(z)^{T}(-x+z+y-z) \\
& \geq f(z) + \nabla f(z)^{T}(x-z) + t\nabla f(z)^{T}(y-x) \\
& \geq f(z) + \nabla f(z)^{T}(x-z) + t\nabla f(z)^{T} \dfrac{(z-x)}{t} \\
& \geq f(z) + \nabla f(z)^{T}(x-z) - \nabla f(z)^{T}(x-z) \\
& = f(z) \\
& = f((1-t)x + ty) .
\end{align}

Portanto, a função $f$ é convexa em $C$.
\end{proof}


A seguir, apresentamos alguns resultados necessários para demonstrar o \Cref{teo:hessiana_convexidade}.


\begin{teo}(\textbf{Taylor de Segunda Ordem}) {\cite[p.26]{Ademir2013}} \label{teo:Taylor_segunda_ordem}
Se $f:\RR^{n} \rightarrow \RR$ é uma função duas vezes diferenciável e $\xbar \in \RR^{n}$, então
\[
f(x) = f(\xbar) + \nabla f(\xbar)^{T}(x-\xbar) + \dfrac{1}{2}(x-\xbar)^{T}\nabla^{2} f(\xbar)(x-\xbar) + r(x),
\] 
com $\lim_{x \rightarrow \xbar}\dfrac{r(x)}{\Vert x - \xbar \Vert^{2}} = 0$.
\end{teo}


\begin{teo}(\textbf{Taylor com Resto de Lagrange}) {\cite[p.26]{Ademir2013}} \label{teo:Taylor_com_resto_lagrange}
Considere $f: \RR^{n} \rightarrow \RR$ uma função de classe $\mathcal{C}^{1}$ e $\xbar, d \in \RR^{n}$. Se $f$ é duas vezes diferenciável no segmento $(\xbar , \xbar + d)$, então existe $t \in (0,1)$ tal que
\[
f(\xbar + d) = f(\xbar) + \nabla f(\xbar)^{T} d + \dfrac{1}{2}d^{T}\nabla^{2} f(\xbar + td)d .
\]
\end{teo}


\begin{lema} \label{lema:auxiliar_para_teo_hessiana_convexidade}
Sejam $C \subset \RR^{n}$ convexo, $x \in \bar{C}$ e $y \in \interior C$. Então, $(x,y] \subset \interior C$.
\end{lema}
\begin{proof}
Por demonstrar.
\end{proof}

%{\cite[p.60]{Ademir2013}} 
\begin{teo} \label{teo:hessiana_convexidade}
Sejam $f:\RR^{n} \rightarrow \RR $ uma função de classe $\mathcal{C}^{2}$ e $C \subset \RR^{n}$ um conjunto convexo com interior não vazio. A função $f$ é convexa em $C$ se, e somente se, a Hessiana $\nabla^{2} f(x)$ é semidefinida positiva para todo $x \in C$.
\end{teo}
\begin{proof}
Considere $x \in \interior C$. Então, dado $d \in \RR^{n}$ temos que $x+td \in C$, para $t$ suficientemente pequeno. Portanto, pela convexidade de $f$ e pelo \Cref{teo:diferenciabilidade_e_convexidade}, temos
\[
f(x+td) \geq f(x) + t\nabla f(x)^{T} d ,
\]
e assim, 
\[ \label{eq:1_teo_hessiana_convexidade}
f(x+td) -f(x) - t\nabla f(x)^{T} d \geq 0 . 
\]

Aplicando o \Cref{teo:Taylor_segunda_ordem} para $f(x+td)$, temos
\[
f(x+td) = f(x) + t\nabla f(x)^{T}d + \dfrac{t^{2}}{2}d^{T} \nabla^{2} f(x)d + r(t^{2}) ,
\]
e substituindo em \eqref{eq:1_teo_hessiana_convexidade}, obtemos
\[
\dfrac{t^{2}}{2}d^{T} \nabla^{2} f(x)d + r(t^{2}) \geq 0 ,
\]
com $\lim_{t \rightarrow 0} \dfrac{r(t^{2})}{t^{2}} = 0$. 

Dividindo por $t^{2}$ e passando o limite com $t \rightarrow 0$, temos
\[
d^{T} \nabla^{2} f(x)d \geq 0 .
\]

Agora, considere $x \in C$ arbitrário. Como existe $y \in \interior C$, o \Cref{lema:auxiliar_para_teo_hessiana_convexidade} garante que todos os pontos do segmento $(x,y] \subset \interior C$. Então, pelo que acabamos de provar, dados $d \in \RR^{n}$ e $t\in (0,1]$, vale
\[
d^{T} \nabla^{2} f((1-t)x+ty)d \geq 0 .
\]

Fazendo $t \rightarrow 0^{+}$ e usando a continuidade de $\nabla^{2} f$, obtemos
\[
d^{T} \nabla^{2} f(x)d \geq 0,
\]
para todo $x \in C$.

Reciprocamente, dados $x \in C$ e $d \in \RR^{n}$ tal que $x+d \in C$, pelo \Cref{teo:Taylor_com_resto_lagrange},
\[
f(x+d) = f(x) + \nabla f(x)^{T}d + \dfrac{1}{2}d^{T} \nabla^{2} f(x+td)d
\]
para algum $t \in (0,1)$. Como $\nabla ^{2}f(x+td) \geq 0$, concluímos que 
\[
f(x+d) \geq f(x) + \nabla f(x)^{T}d .
\]

Logo, pelo \Cref{teo:diferenciabilidade_e_convexidade}, $f$ é convexa.
\end{proof}

Em geral, a função objetivo dos problemas de SVM serão funções quadráticas. Assim, faz-se necessário mostrar que a função quadrática é convexa. Tal resultado é apresentado a seguir.

%\cite{Gondzio2018}
\begin{teo}
Seja $C \in \RR^{n}$ um conjunto convexo e $Q$ uma matriz quadrada. Seja $f:C \rightarrow \RR$ tal que $f(x) = x^{T}Qx$ é uma função quadrática. Então, $f$ é convexa se, e somente se, $Q$ é semidefinida positiva.
\end{teo}
\begin{proof}
Por demonstrar.
\end{proof}





\newpage

\printbibliography
\end{document}